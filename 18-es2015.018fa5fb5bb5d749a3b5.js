(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{wtaK:function(l,n,e){"use strict";e.r(n);var o=e("8Y7J");class t{constructor(){}ngOnInit(){}}class a{}var i=e("pMnS"),u=e("oBZk"),r=e("ZZ/e"),s=o.pb({encapsulation:0,styles:[[""]],data:{}});function c(l){return o.Hb(0,[(l()(),o.rb(0,0,null,null,10,"ion-header",[],null,null,null,u.K,u.m)),o.qb(1,49152,null,0,r.y,[o.h,o.k,o.x],null,null),(l()(),o.rb(2,0,null,0,8,"ion-toolbar",[],null,null,null,u.V,u.x)),o.qb(3,49152,null,0,r.yb,[o.h,o.k,o.x],null,null),(l()(),o.rb(4,0,null,0,3,"ion-buttons",[["slot","start"]],null,null,null,u.A,u.c)),o.qb(5,49152,null,0,r.i,[o.h,o.k,o.x],null,null),(l()(),o.rb(6,0,null,0,1,"ion-menu-button",[],null,null,null,u.P,u.s)),o.qb(7,49152,null,0,r.O,[o.h,o.k,o.x],null,null),(l()(),o.rb(8,0,null,0,2,"ion-title",[],null,null,null,u.U,u.w)),o.qb(9,49152,null,0,r.wb,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Research"])),(l()(),o.rb(11,0,null,null,172,"ion-content",[],null,null,null,u.I,u.k)),o.qb(12,49152,null,0,r.r,[o.h,o.k,o.x],null,null),(l()(),o.rb(13,0,null,0,17,"ion-card",[],null,null,null,u.F,u.d)),o.qb(14,49152,null,0,r.j,[o.h,o.k,o.x],null,null),(l()(),o.rb(15,0,null,0,4,"ion-card-header",[],null,null,null,u.C,u.f)),o.qb(16,49152,null,0,r.l,[o.h,o.k,o.x],null,null),(l()(),o.rb(17,0,null,0,2,"ion-card-title",[["style","font-size:2em;"]],null,null,null,u.E,u.h)),o.qb(18,49152,null,0,r.n,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Learning Locomotion using a Keyframe-based System"])),(l()(),o.rb(20,0,null,0,10,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(21,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.rb(22,0,null,0,2,"ion-card-subtitle",[["style","font-size:1.5em;"]],null,null,null,u.D,u.g)),o.qb(23,49152,null,0,r.m,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Ethan Wilson"])),(l()(),o.rb(25,0,null,0,2,"ion-card-subtitle",[["style","font-size:1.5em;"]],null,null,null,u.D,u.g)),o.qb(26,49152,null,0,r.m,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Greg Turk"])),(l()(),o.rb(28,0,null,0,2,"ion-card-subtitle",[["style","font-size:1.5em;"]],null,null,null,u.D,u.g)),o.qb(29,49152,null,0,r.m,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Yunbo Zhang"])),(l()(),o.rb(31,0,null,0,5,"ion-card",[["button","true"],["color","tertiary"],["download","draft.docx"],["href","assets/loco/draft.docx"]],null,null,null,u.F,u.d)),o.qb(32,49152,null,0,r.j,[o.h,o.k,o.x],{button:[0,"button"],color:[1,"color"],download:[2,"download"],href:[3,"href"]},null),(l()(),o.rb(33,0,null,0,3,"ion-card-content",[["style","text-align:center;"]],null,null,null,u.B,u.e)),o.qb(34,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.rb(35,0,null,0,1,"h2",[],null,null,null,null,null)),(l()(),o.Gb(-1,null,["Download Paper"])),(l()(),o.rb(37,0,null,0,15,"ion-card",[],null,null,null,u.F,u.d)),o.qb(38,49152,null,0,r.j,[o.h,o.k,o.x],null,null),(l()(),o.rb(39,0,null,0,4,"ion-card-header",[],null,null,null,u.C,u.f)),o.qb(40,49152,null,0,r.l,[o.h,o.k,o.x],null,null),(l()(),o.rb(41,0,null,0,2,"ion-card-title",[],null,null,null,u.E,u.h)),o.qb(42,49152,null,0,r.n,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Introduction"])),(l()(),o.rb(44,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(45,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nAnimated motion of a digital character / model is derived from a few different methods which depend on the simulation environment. Traditional animation, applied in video games, animated film, etc., keeps a record of keyframes, or values for the rotation of joints within a model's underlying skeleton. These keyframes correspond to different points in time, and the program interpolates between them to generate the animation. These keyframes are typically either animated by hand or obtained through motion capture technology. "])),(l()(),o.rb(47,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(48,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nAnimations that must conform to the laws of physics, i.e. existing within physics simulations, must be animated in a different manner. Real humans / creatures perform an infinite amount of subconscious motions to preserve balance and save energy, but it would take an exorbitant amount of time for a human animator to replicate those balance changes using keyframes in a physical simulation. Without those corrections, realistic-looking animation loops might lose their stability over time, resulting in the agent falling over, and successfully human-animated motion would likely look robotic or simulated. "])),(l()(),o.rb(50,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(51,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nMy goal was to create a physics-based animation system that could import custom models and generate stable walk cycles [i.e. completing multiple cycles without falling over, yet still moving in the forward direction] within an affordable amount of time. "])),(l()(),o.rb(53,0,null,0,7,"ion-card",[["color","secondary"]],null,null,null,u.F,u.d)),o.qb(54,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(55,0,null,0,5,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(56,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.rb(57,0,null,0,3,"ion-card",[["color","dark"],["style","padding:10px"]],null,null,null,u.F,u.d)),o.qb(58,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(59,0,null,0,1,"video",[["controls","controls"],["style","display:block;width:100%;height:auto;margin-left:auto;margin-right:auto;"]],null,null,null,null,null)),(l()(),o.rb(60,0,null,null,0,"source",[["src","assets/loco/video/quad_good_walk.mp4"],["type","video/mp4"]],null,null,null,null,null)),(l()(),o.rb(61,0,null,0,12,"ion-card",[],null,null,null,u.F,u.d)),o.qb(62,49152,null,0,r.j,[o.h,o.k,o.x],null,null),(l()(),o.rb(63,0,null,0,4,"ion-card-header",[],null,null,null,u.C,u.f)),o.qb(64,49152,null,0,r.l,[o.h,o.k,o.x],null,null),(l()(),o.rb(65,0,null,0,2,"ion-card-title",[],null,null,null,u.E,u.h)),o.qb(66,49152,null,0,r.n,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Related Work"])),(l()(),o.rb(68,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(69,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,['\nTraditionally, the solution for animating with respect to physical constraints has been to employ some heuristic optimization algorithm. Heuristic algorithms do not search for a single perfect solution; instead they approximate a solution that is "good enough" and can be found in a reasonable amount of time. These algorithms are especially effective in locomotion because of the large problem space (there are infinite variations of acceptable forms of locomotion). Evolutionary algorithms work particularly well as they tend to avoid local maxima and can indefinitely continue searching for a more efficient solution. For example, Kodjabachian used an evolutionary approach to build a neural network controller for simulated two-dimensional insects. These insects were able to learn locomotion and perform higher level behaviors, such as obstacle avoidance and an odor-gradient food detection system[1]. '])),(l()(),o.rb(71,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(72,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nRecent improvements in computational technology have opened the gateway to apply more computationally expensive algorithms. Reinforcement learning (RL), a subset of machine learning that balances an agent's environmental exploration vs. exploitation, has become an especially popular method. There are various existing implementations of RL locomotion training, and some have extended functionality for various scenarios. An example is Simbicon, a three-dimensional biped locomotion controller that incorporated the ability to react to varied terrain and outside forces[2]. However, these solutions remain too computationally expensive to perform without a dedicated setup. While a research lab might be able to afford a week of training on a dedicated multi-GPU system, I am restricted to using a mid-range laptop. "])),(l()(),o.rb(74,0,null,0,7,"ion-card",[["color","secondary"]],null,null,null,u.F,u.d)),o.qb(75,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(76,0,null,0,5,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(77,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.rb(78,0,null,0,3,"ion-card",[["color","dark"],["style","padding:10px"]],null,null,null,u.F,u.d)),o.qb(79,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(80,0,null,0,1,"video",[["controls","controls"],["style","display:block;width:100%;height:auto;margin-left:auto;margin-right:auto;"]],null,null,null,null,null)),(l()(),o.rb(81,0,null,null,0,"source",[["src","assets/loco/video/crab_run.mp4"],["type","video/mp4"]],null,null,null,null,null)),(l()(),o.rb(82,0,null,0,9,"ion-card",[],null,null,null,u.F,u.d)),o.qb(83,49152,null,0,r.j,[o.h,o.k,o.x],null,null),(l()(),o.rb(84,0,null,0,4,"ion-card-header",[],null,null,null,u.C,u.f)),o.qb(85,49152,null,0,r.l,[o.h,o.k,o.x],null,null),(l()(),o.rb(86,0,null,0,2,"ion-card-title",[],null,null,null,u.E,u.h)),o.qb(87,49152,null,0,r.n,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["My Approach"])),(l()(),o.rb(89,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(90,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nMy implementation used a Genetic Algorithm (GA), an evolutionary algorithm in which various DNA compete within a population that evolves over time. In order to save computation time and limit the problem's search space, I opted to model my DNA after the keyframe system present in traditional animation. Thus, my solution is prone to the problems animators face (discussed in the introduction), but it is able to achieve significantly more precise values than any human animator could. "])),(l()(),o.rb(92,0,null,0,7,"ion-card",[["color","secondary"]],null,null,null,u.F,u.d)),o.qb(93,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(94,0,null,0,5,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(95,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.rb(96,0,null,0,3,"ion-card",[["color","dark"],["style","padding:10px"]],null,null,null,u.F,u.d)),o.qb(97,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(98,0,null,0,1,"video",[["controls","controls"],["style","display:block;width:100%;height:auto;margin-left:auto;margin-right:auto;"]],null,null,null,null,null)),(l()(),o.rb(99,0,null,null,0,"source",[["src","assets/loco/video/tall_walk.mp4"],["type","video/mp4"]],null,null,null,null,null)),(l()(),o.rb(100,0,null,0,21,"ion-card",[],null,null,null,u.F,u.d)),o.qb(101,49152,null,0,r.j,[o.h,o.k,o.x],null,null),(l()(),o.rb(102,0,null,0,4,"ion-card-header",[],null,null,null,u.C,u.f)),o.qb(103,49152,null,0,r.l,[o.h,o.k,o.x],null,null),(l()(),o.rb(104,0,null,0,2,"ion-card-title",[],null,null,null,u.E,u.h)),o.qb(105,49152,null,0,r.n,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Skel Controller within DART"])),(l()(),o.rb(107,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(108,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nThe physics simulation I chose to use was the Dynamic Animation and Robotics Toolkit (DART). DART is an open-source library that was written at the Georgia Institute of Technology collaboratively by the Graphics Lab and Humanoid Robotics Lab[3]. I chose DART both because it is an ideal environment for simulating rigid body systems and because of my advisors' experience with the program. "])),(l()(),o.rb(110,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(111,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nDART's functionality includes a customizable controller for every rigid body entity within the simulation. This controller can be used to dynamically apply torque to the joints of the rigid body during the simulation's runtime. The controller I implemented was an extension of the Stable Proportional-Derivative (SPD) controller[4]. In the context of a rigidbody system, a SPD controller provides stable tracking to a target pose. This is achieved by applying a control force to the rigid body's joints, computed using the joint's current kinematics, the target position, and the simulation's timestep. A proportional gain (tracking strength) and derivative gain (damping strength) are included in the equation used and can be changed to tweak the tracking intensity. SPD controllers differ from traditional Proportional-Derivative (PD) controllers in their calculation; SPD use a computed estimate of the simulation's next time-step rather than the current time-step. This method ensures stability when undergoing large motions or high time steps, whereas the traditional PD controller may break down. "])),(l()(),o.rb(113,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(114,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nMy controller utilizes the SPD controller's computations for all torque calculations. At the initialization phase of the simulation, my controller is fed a list of tracking poses. Then, when applicable, these poses are mirrored and/or interpolated to generate a large cycle of poses for the controller to use. "])),(l()(),o.rb(116,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(117,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nTo mirror a pose, specific values within the pose (referred to as degrees of freedom, explained in-depth in a later section) that map from left to right are transferred to the opposite side, and vice versa. The code to mirror depends on the model at hand. Interpolation can be applied generally to any model. To interpolate, two adjacent poses from within the loop are chosen. Multiple intermediate poses are generated by linearly interpolating between the two chosen poses. These generated poses are then added to a new list which ends up being much larger. This process is repeated between each adjacent pair of poses, with the interpolated poses being added to the same new list, which finally replaces the initial list of poses. "])),(l()(),o.rb(119,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(120,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nAt runtime, the controller tracks to a pose and constantly computes an error value between the model's current position and the pose being tracked. Once this error drops beneath a specific value, the target pose shifts to the next pose in the list. Using error to determine when to track a new pose differs from animation's traditional keyframe approach, where the tracking of keyframes are based completely on time. My approach generated smoother motions than a linear time-based system would; additionally, this system discouraged unexpected behaviors by adding physical constraints to the rigidbody. If a model was unable to reach its next pose (from falling over and being blocked by the ground, or any other reason), it would get stuck, whereas a strictly time-based keyframe system's model would ignore the fact that it was stuck and continue attempting to reach poses, resulting in unnatural, spastic motion. "])),(l()(),o.rb(122,0,null,0,7,"ion-card",[["color","secondary"]],null,null,null,u.F,u.d)),o.qb(123,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(124,0,null,0,5,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(125,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.rb(126,0,null,0,3,"ion-card",[["color","dark"],["style","padding:10px"]],null,null,null,u.F,u.d)),o.qb(127,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(128,0,null,0,1,"video",[["controls","controls"],["style","display:block;width:100%;height:auto;margin-left:auto;margin-right:auto;"]],null,null,null,null,null)),(l()(),o.rb(129,0,null,null,0,"source",[["src","assets/loco/video/quad_mirror_pose.mp4"],["type","video/mp4"]],null,null,null,null,null)),(l()(),o.rb(130,0,null,0,9,"ion-card",[],null,null,null,u.F,u.d)),o.qb(131,49152,null,0,r.j,[o.h,o.k,o.x],null,null),(l()(),o.rb(132,0,null,0,4,"ion-card-header",[],null,null,null,u.C,u.f)),o.qb(133,49152,null,0,r.l,[o.h,o.k,o.x],null,null),(l()(),o.rb(134,0,null,0,2,"ion-card-title",[],null,null,null,u.E,u.h)),o.qb(135,49152,null,0,r.n,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Genetic Algorithm"])),(l()(),o.rb(137,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(138,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nA genetic algorithm (GA) is a framework for solving optimization problems which is based upon the process of natural selection found in biological evolution. Different solutions to the problem, referred to as genomes, are initially randomly generated by the algorithm. These genomes are stored in a list of fixed size known as the population. The algorithm progresses by alternating between evolutionary and evaluation stages. At each evaluation stage, every genome in the population is tested against a fitness function and given a score which corresponds to how effectively they have solved the problem. The evolutionary stage typically involves a combination of crossover and mutation. Crossover is a process in which two genomes, now known as the parents, are selected from the population and their information is combined to produce some number of children. Mutation takes in a single genome sequence and slightly changes some of its values, creating a slightly different solution to the problem. Crossover and mutations are performed on the population in an effort to produce better solutions that can be used in later generations. At the end of each evolutionary stage, the population, which may contain extra DNA from crossover, must be culled back to its original size. "])),(l()(),o.rb(140,0,null,0,7,"ion-card",[["color","secondary"]],null,null,null,u.F,u.d)),o.qb(141,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(142,0,null,0,5,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(143,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.rb(144,0,null,0,3,"ion-card",[["color","dark"],["style","padding:10px"]],null,null,null,u.F,u.d)),o.qb(145,49152,null,0,r.j,[o.h,o.k,o.x],{color:[0,"color"]},null),(l()(),o.rb(146,0,null,0,1,"video",[["controls","controls"],["style","display:block;width:100%;height:auto;margin-left:auto;margin-right:auto;"]],null,null,null,null,null)),(l()(),o.rb(147,0,null,null,0,"source",[["src","assets/loco/video/crab_flipped.mp4"],["type","video/mp4"]],null,null,null,null,null)),(l()(),o.rb(148,0,null,0,35,"ion-card",[],null,null,null,u.F,u.d)),o.qb(149,49152,null,0,r.j,[o.h,o.k,o.x],null,null),(l()(),o.rb(150,0,null,0,4,"ion-card-header",[],null,null,null,u.C,u.f)),o.qb(151,49152,null,0,r.l,[o.h,o.k,o.x],null,null),(l()(),o.rb(152,0,null,0,2,"ion-card-title",[],null,null,null,u.E,u.h)),o.qb(153,49152,null,0,r.n,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["Data Storage"])),(l()(),o.rb(155,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(156,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nI chose to represent my DNA as a list of target poses for the controller within DART to use. Every pose was a list of values, and each value corresponded to the rotation value of one degrees of freedom (DOF) within the model. "])),(l()(),o.rb(158,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(159,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nA model is composed of various body sections, which are connected by joints. There are multiple joint classifications to choose from, with the more complicated joint types having more DOFs. A DOF is a single value that corresponds to one axis of rotation in the world (the axis can be of any orientation, however). The types of joints are as follows: "])),(l()(),o.rb(161,0,null,0,16,"ion-list",[["color","tertiary"],["lines","none"],["style","margin:auto;width:80%;"]],null,null,null,u.O,u.q)),o.qb(162,49152,null,0,r.L,[o.h,o.k,o.x],{lines:[0,"lines"]},null),(l()(),o.rb(163,0,null,0,2,"ion-item",[],null,null,null,u.M,u.o)),o.qb(164,49152,null,0,r.E,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\n1. Revolute joints contain one DOF. An example would be a human\ufffds elbow, which can only rotate along the bicep\ufffds orientation. Notice, however, that as the shoulder moves the elbow\ufffds global rotation also changes, so the revolute joint\ufffds constraint is only local to the body part it is placed on. "])),(l()(),o.rb(166,0,null,0,2,"ion-item",[],null,null,null,u.M,u.o)),o.qb(167,49152,null,0,r.E,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\n2. Universal joints contain two DOFs. An example is the human knee, which can rotate along the thigh\ufffds orientation or rotate slightly along the calf\ufffds axis. It can not rotate all the way around the thigh due to having two axes of rotation in a 3D space. "])),(l()(),o.rb(169,0,null,0,2,"ion-item",[],null,null,null,u.M,u.o)),o.qb(170,49152,null,0,r.E,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\n3. Euler joints contain three DOFs, and can rotate along all axes. An example is the shoulder. Notice that the shoulder still has value constraints. It can rotate along every axis, but the extent of rotation is limited by the person\ufffds flexibility. "])),(l()(),o.rb(172,0,null,0,2,"ion-item",[],null,null,null,u.M,u.o)),o.qb(173,49152,null,0,r.E,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\n4. Free joints connect two separate objects and contain three DOFs for rotation and three DOFs for position. These joints can not be directly manipulated and exist solely to inform the simulation. "])),(l()(),o.rb(175,0,null,0,2,"ion-item",[],null,null,null,u.M,u.o)),o.qb(176,49152,null,0,r.E,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\n5. Weld joints have zero DOFs and rigidly connect two body parts. These help to create more complex geometry by combining primitive shapes. "])),(l()(),o.rb(178,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(179,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nLimiting the number of DOFs on a model would decrease computation time, so models were created creatively to have the simplest combination of joints while still being able to achieve motion similar to the creatures they were modeled after. In addition, I limited the values of these DOFs within the DNA to have a roughly 90 degree range. This restriction acted similarly to a human shoulder in that without constraints, a shoulder would be able to clip into its attached body. This helped to both keep movement natural and to decrease the problem\ufffds search space significantly. "])),(l()(),o.rb(181,0,null,0,2,"ion-card-content",[],null,null,null,u.B,u.e)),o.qb(182,49152,null,0,r.k,[o.h,o.k,o.x],null,null),(l()(),o.Gb(-1,0,["\nTo save additional computation time, for symmetrical models I implemented a mirror system. Each piece of DNA would only contain info for half of a walk cycle, which would then be mirrored over at the beginning of the evaluation phase. This halved the amount of data in the GA, which exponentially decreased search time. "]))],function(l,n){l(n,32,0,"true","tertiary","draft.docx","assets/loco/draft.docx"),l(n,54,0,"secondary"),l(n,58,0,"dark"),l(n,75,0,"secondary"),l(n,79,0,"dark"),l(n,93,0,"secondary"),l(n,97,0,"dark"),l(n,123,0,"secondary"),l(n,127,0,"dark"),l(n,141,0,"secondary"),l(n,145,0,"dark"),l(n,162,0,"none")},null)}function h(l){return o.Hb(0,[(l()(),o.rb(0,0,null,null,1,"app-locomotion",[],null,null,null,c,s)),o.qb(1,114688,null,0,t,[],null,null)],function(l,n){l(n,1,0)},null)}var d=o.nb("app-locomotion",t,h,{},{},[]),b=e("SVse"),m=e("s7LF"),p=e("iInd");e.d(n,"LocomotionPageModuleNgFactory",function(){return f});var f=o.ob(a,[],function(l){return o.zb([o.Ab(512,o.j,o.Z,[[8,[i.a,d]],[3,o.j],o.v]),o.Ab(4608,b.j,b.i,[o.s,[2,b.p]]),o.Ab(4608,m.c,m.c,[]),o.Ab(4608,r.a,r.a,[o.x,o.g]),o.Ab(4608,r.Cb,r.Cb,[r.a,o.j,o.p]),o.Ab(4608,r.Fb,r.Fb,[r.a,o.j,o.p]),o.Ab(1073742336,b.b,b.b,[]),o.Ab(1073742336,m.b,m.b,[]),o.Ab(1073742336,m.a,m.a,[]),o.Ab(1073742336,r.Ab,r.Ab,[]),o.Ab(1073742336,p.o,p.o,[[2,p.t],[2,p.m]]),o.Ab(1073742336,a,a,[]),o.Ab(1024,p.k,function(){return[[{path:"",component:t}]]},[])])})}}]);